<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Daoyuan's Fast Robots</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Daoyuan Jin</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="..." /></span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#home">Home</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#aboutme">aboutme</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab1">lab1</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab2">lab2</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab3">lab3</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab4">lab4</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab5">lab5</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab6">lab6</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab7">lab7</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab8">lab8</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab9">lab9</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#lab10">lab10</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- Home -->
            <section class="resume-section" id="home">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        ECE5160
                        <span class="text-primary">Fast Robots</span>
                    </h1>
                    <div class="subheading mb-5">
                        Daoyuan Jin · ECE Grad Student ·
                        <a href="dj368@cornell.edu">dj368@cornell.edu</a>
                    </div>
                    <p class="lead mb-5">Hi there. Welcome to my webpage for ECE 5160 Fast Robots. Most of the links here are still under construction...</p>
                    <div class="social-icons">
                        <a class="social-icon" href="#!"><i class="fab fa-linkedin-in"></i></a>
                        <a class="social-icon" href="#!"><i class="fab fa-github"></i></a>
                        <a class="social-icon" href="#!"><i class="fab fa-twitter"></i></a>
                        <a class="social-icon" href="#!"><i class="fab fa-facebook-f"></i></a>
                    </div>
                </div>
            </section>
            <hr class="m-0" />            
            
            <!-- About me -->
            <section class="resume-section" id="aboutme">
                <div class="resume-section-content">
                    <h2 class="mb-5">about me</h2>
                    <p>I'm a graudate student in CAIR lab, Cornell AgriTech. During my free time I'd like to go outdoors.</p>
                    <p class="mb-0">To be continued.</p>
                </div>
            </section>
            <hr class="m-0" />

            <!-- LAB 1 -->
            <section class="resume-section" id="lab1">
                <div class="resume-section-content">
                    <h2 class="mb-5">LAB1: The Artemis board and Bluetooth</h2>

                    <h2 class="mb-5">PART1: The Artemis board</h2>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Lab Objective</h3>
                            <p>This part acts like a introduction to the Artemis board and the Arduino IDE. By testing examples in the library, we gradually become familiar with the process of using Arduino boards.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Prelab</h3>
                            <p>Setup Arduino environment is reletively simple. We first install the Arduino IDE and install the Sparkfun Appollo3 library accodring to the setup instructions. Then hook the Artemis board up to the computer, select a corresponding borad. Now we are good to uploud examples.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Example: Blink it Up</h3>
                            <p>As the first code we tested, the "Blink it up" example turns the LED on and off periodically.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/353Emwl2QN0" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Example: Serial</h3>
                            <p>In this example we test the serial output and the function of serial monitor. When we send an input to interface, the Artemis board would send it back to the serial monitor as the video shows.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/CmzyefaJIZQ" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Example: analogRead</h3>
                            <p>In the third example we test the fuction of temperature sensor and the board's ability to read analog values. As you can see in the video, the original "temp" value is around 33.9 Celsius. I hold the board still for a while and the reading goes up a little to 34.0 Celsius.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/lKkR116vCI0" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Example: MicrophoneOutput</h3>
                            <p>This example tests the function of microphone unit. It has the ability to print out the loudest frequency the board received.</p>
                            <p>Please turn down your volume a little bit before click the video. Thanks for your understanding!</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/2xZcox5h8T4" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">5000-level: Electronic Tuner</h3>
                            <p>On top of the MicrophoneOutput example, I write a code to make the board act like a electronic tuner. As the "Ode To Joy" goes, the board will identify the frequency of each note and print out a "C", "E" or "F" whenever it encounter one. (Unfortunately there are no "A"s in Ode To Joy.) And it will blink for 0.5 second when it hear a "C", which can be noticed in the video.</p>
                            <p>The logic behind is pretty simple. We can search on the internet for the frequencies of musical notes. For example, C5 is 523.25Hz. But in my case, it turns out to be around 526Hz. Due to the deviation in pitch accuracy, and certain overtones produced during the instrument performance, it's hard to fully encode all the musical notes. But it can be an interesting task in the future practice.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/Ku1s3unY-Mc" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>

                    <h2 class="mb-5">PART2: Bluetooth</h2>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Lab Objective</h3>
                            <p>The objective of this part is to establish communication between PC and the Artemis board through the Bluetooth connection. On PC side we use Python3 in Jupyter notebook, while on the Artemis side we use the Arduino programming language, which is a simplified version of C/C++. We will get familiar with the process of sending data via Bluetooth that will be useful in future labs.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Prelab</h3>
                            <div class="subheading mb-3">1. Setup</div>
                            <p>PC side: Install Python, Create and Activate Virtual Environment, Install Python Packages, Using Jupyter Server.</p>
                            <p>Artemis side: Install ArduinoBLE, Using ble_arduino.ino.</p>
                            <p>For this 2024 spring class, I'm using Win11 and I went through all the setups and tasks without using WSL, although I still spent some time trying to install it. It turns out that all the errors popped up can be fixed inside Windows11.</p>
                            <div class="subheading mb-3">2. Codebase</div>
                            <p>The Python and Artemis packages are provided to establish a basic communication between PC and the Artemis board through BLE. We will modify on top of the codebase</p>
                            <p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Workflow</h3>
                            <p>To start with, let me priefly introduce the general workflow of writing a new bluetooth function.</p>
                            <p>On Artemis side: Twosteps. First, add CommandTypes, so that the board can handle commands accordingly. Second, add a case under switch function, define the specific function inside this case. Typical output steps are Serial.print() and tx_characteristic_string.writeValue(), which print out to the serial monitor and send message via bluetooth, respectively.</p>
                            <p>On PC side: add CMD(Enum) in cmd_types.py just like CommandTypes</p>
                            <code>class CMD(Enum):<br>
                                    PING = 0<br>
                                    SEND_TWO_INTS = 1<br>
                                    SEND_THREE_FLOATS = 2<br>
                                    ECHO = 3<br>
                            </code>
                            <p>Use ble.send_command() and ble.receive_string() pair to send and receive message. Or use ble.start_notify() to start a notification handler, which we will cover below.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Establish Bluetooth Connection</h3>                            
                            <p>First, upload the ble_arduino.ino into the board. Once finished, the MAC address will be printed in the serial monitor. Replace the first line in connection.yaml file with this address.</p>
                            <p>Second, in order to differentiate from other students' boards, generate a new Universally Unique Identifier (UUID) by using lines below.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/uuid.png" alt="">
                            <p>Replace the second line in connection.yaml file with this UUID. Replace the BLEService UUID in ble_arduino.ino with this UUID too.</p>
                            <p>Last, run the demo.ipynb notebook to make sure Bluetooth connection is established.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/rh8XcAS1fWI" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task1: ECHO</h3>
                            <p>First we write a "ECHO" command to let Artemis send back a signitured string to test the send & receive functions via bluetooth. On PC side the code and result looks like this:</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/echo_py.png" alt="">
                            <p>On Artemis side the code looks like this:</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/echo_ar.png" alt="">
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task2: GET TIME MILLIS</h3>
                            <p>On top of string transmission, we use millis() function in Artemis to send back the time information. On PC side the code and result looks like this:</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/millis_py.png" alt="">
                            <p>On Artemis side the code looks like this:</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/millis_ar.png" alt="">
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task3: Notification Handler</h3>
                            <p>When we use ble.receive_string to receive information from Artemis, the entire Python program enters a waiting state and does not handle other tasks, which is very time-consuming in practical use. To address this issue, we developed the Notification Handler function. When we activate this function, the callback function is called to process the information every time Bluetooth receives it, and it does not occupy the thread when there is no new information. The following video shows the specific implementation and result:</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/aDigYVjs5FY" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>


                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task4: Data Transfer Rate</h3>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/rate.png" alt="">
                            <p>Now we write a loop in Artemis (show in the picture above) that gets the current time in milliseconds and sends it to PC to be received and processed by the notification handler. Collect for around 5s and calculate the effective data transfer rate. As the video shows, during 5s the PC received around 200 timestamps, thus indicating a data transfer rate 40/s.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/KvO5sbs1OFo" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task5: SEND TIME DATA</h3>                            
                            <p>We also have different ways to transfer data. Instead of sending message once a time, we can store it in an array first. Once we send request from PC for the timestamps then the board send all its data to PC.</p>
                            <p>To address this we first add the following lines in the loop (besides default write_data() and read_data()) to store timestamps. Once the array is full we overwrite old data with new data while always preserving the index information of the data in the array, allowing us to know from which position to start reading when sending data.</p>
                            <code>
                                timestamps[myindex] = (int)millis();<br>
                                myindex += 1;<br>
                                if (myindex > MAXSIZE){<br>
                                myindex = 0; <br>
                                }                                <br>
                                write_data(); // Send data<br>
                                read_data(); // Read data<br>
                            </code>
                            <p>After that we define "case: SEND_TIME_DATA" in Artemis and "SEND TIME DATA" in Python as shown in the video. In this example, I define the array length to be 255, each time we want to get timestamps from Artemis, it will send us this array in sequence.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/lUwt-pHpjF8" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task6: GET TEMP READINGS</h3>                                                        
                            <p>To get synchronized time and temperature data, we simply add the corresponding part for temperature as timestamp in Task5. The video shows that both Artemis and PC fulfilled the requirement.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/PFC5NmynVaM" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task7: Discussion</h3>
                            
                            <p>One data at a time: this method doesn't count on the RAM, if we are dealing with single piece large data and small RAM, we'd better choose this method.</p>
                            <p>Sending data after storage: this method has the ability to record denser data points campared with the former, also this method allows the robot temporarily disconnecting with PC (although in our case it's not possible).</p>
                            <p>The Artemis board has 384 kB of RAM. If we use all of the RAM to store our data, and one sample value is 32-bits (string in utf-8), then it will be 12000 values storage.</p>
                            
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">5000-level: Effective Data Rate And Overhead</h3>
                            <p>To test the relationship between data length and data rate, I modified the ECHO function to an exact sentback mode. Then I proceed a expriment to calculate the time it takes for transmitting one byte of data.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/data_rate.jpg" alt="">
                            <p>The result graph shows that by increasing the data length, the effective data rate gradually increase as well.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">5000-level: Reliability</h3> 
                            <p>By comparing the stored data in Artemis and PC, I found that the computer read all the data published by Artemis, even if Artemis sends at a higher rate.</p>
                        </div>
                    </div>

                </div>
            </section>
            <hr class="m-0" />

            <!-- LAB 2 -->
            <section class="resume-section" id="lab2">
                <div class="resume-section-content">
                    <h2 class="mb-5">LAB2: IMU</h2>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Lab Objective</h3>
                            <p>In the second lab we add a IMU to our robot (Artemis board). We record and process the Accelerometer and Gyroscope data, and implement a low pass filter to improve the data quality. We also test the RC car to get a sense of future work.</p>
                        </div>
                    </div>


                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task1: Set up the IMU</h3>
                            <p>We use the Sparkfun breakout board as IMU, and install the SparkFun 9DOF IMU Breakout - ICM 20948 - Arduino Library through the Arduino library manager.</p>
                            <p>Connect the IMU to the Artemis board using the QWIIC cable.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/imu.jpg" alt="">
                            <p>Run basic example to read the sensor data. As you can see in this video, Accelerometer and Gyroscope both have X, Y, Z three dementional data. As I rotate, flip and accelerate the board, data change accordingly. Next we will use functions to convert these data to the gesture of the board.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/PwWLxMT4WnA" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>The definition of AD0_VAL is confusing. In my case I have to set to 0 to get the sensor data. However, on my board the ADDR jumper is soldered, which indicate I should use 1 to get default setting. Anyway, "0" works.</p>
                            <p>Furthermore, as suggested in the instruction, I add a blink function in the setup of Artemis board. You can see the blue light blink three times on start-up.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task2: Accelerometer</h3>
                            <p>First we use the following equations the convert Accelerometer data into pitch and roll.</p>
                            <img class="img-fluid img-centered" src="assets/img/lab2/accequation.png" alt="">
                            <p>In Artemis it goes like this:</p>
                            <code>
                                pitch_a = atan2(myICM.accY(),myICM.accZ())*180/M_PI; <br>
                                roll_a  = atan2(myICM.accX(),myICM.accZ())*180/M_PI; <br>
                            </code>
                            <p>To demonstrate the accuracy of Acc data, we show the output at {-90, 0, 90} degrees for pitch and roll respectively.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/pitch_roll_0.png" alt="">
                            <p>Picture1 pitch & roll @ 0 degree </p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/pitch-90.png" alt="">
                            <p>Picture2 pitch @ -90 degree </p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/pitch90.png" alt="">
                            <p>Picture3 pitch @ 90 degree </p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/roll-90.png" alt="">
                            <p>Picture4 roll @ -90 degree </p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/pitch90.png" alt="">
                            <p>Picture5 roll @ 90 degree </p>
                            <p>We can see that there is noise in the data but generally accurate.</p>
                            <p>Upon applying linear regression to the three-point dataset, we observe a minor offset and a slope coefficient below 1.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/measurepitch.jpg" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/measureroll.jpg" alt="">
                            <p>By manipulating the equations in the graph, we can derive the conversion factor. However, this data exhibits significant randomness and is heavily influenced by the desk's incomplete horizontal/vertical alignment during the experiment.</p>
                            <!-- FFT -->
                            <div class="subheading mb-3">Low Pass Filter</div>
                            <p>The accelerometer is noisy. To address this issue by using low pass filter, we first to analyze the noise in the frequency spectrum</p>
                            <p>Here is a helpful reference on <a href="https://pythonnumericalmethods.berkeley.edu/notebooks/chapter24.04-FFT-in-Python.html">Fourier Transform</a>.</p>
                            <p>Take pitch data as an example, I collected nearly a thousand data points while the board was stationary. The collected data and the data after Fourier transformation are shown in the graph.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/pitch.png" alt="">
                            <p>From the results, it appears that the intensity of the noise is much lower than the intensity of the signal. Perhaps there is no need to add another low pass filter.</p>
                            <p>In fact, by checking the datasheet, we found that the IMU has a built-in low-pass filter. However, upon closer examination, I believe that this function is not enabled in the mode we are using.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/builtinLPF1.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/builtinLPF2.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/builtinLPF3.png" alt="">
                            <p>Anyway, we can still add an additional low-pass filter to see its effect.</p>
                            <!-- LPF -->
                            <p>Take pitch as an example, the implementation of a low pass filter is like this:</p>
                            <code>
                                const float alpha = 0.2; <br>
                                pitch_a_LPF[1] = alpha*pitch_a + (1-alpha)*pitch_a_LPF[0]; <br>
                                pitch_a_LPF[0] = pitch_a_LPF[1]; <br>
                            </code>
                            <p>We have a list, pitch_a_LPF[], of two doubles, to record the previous and present pitch data, and implement LPF by adjusting the parameter alpha.</p>
                            <p>Mathematically, the cutoff frequency is defined as the frequency at which the output power of the filter is reduced to half of its maximum value. In terms of the transfer function of the filter, the cutoff frequency is the frequency at which the magnitude response is 0.707 times the maximum magnitude. Here we set alpha=0.2.</p>
                            <p>This is the result of pitch data after applying the low pass filter:</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/pitch_lpf.png" alt="">
                            <p>In this experiment I recorded around 1000 data points. Starting from 500 I added some vibration to the Artemis board. In my opinion, the figure in the frequency spectrum is pretty decent.</p>
                            <p>By contrast, the roll data was recorded simultaniously but without a low pass filter. Shown as below.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/row_wo_lpf.png" alt="">
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task3: Gyroscope</h3>
                            <p>The reading of the gyroscope is the angular acceleration of the XYZ coordinates. By integrating them separately, we can obtain the angles of the three coordinates. Artemis implementation is as follows:</p>
                            <code>
                                pitch_g = pitch_g + myICM.gyrX()*dt; <br>
                                roll_g = roll_g + myICM.gyrY()*dt; <br>
                                yaw_g = yaw_g + myICM.gyrZ()*dt; <br>
                            </code>
                            <p>This video shows the pitch, roll, and yaw data we get from Gyroscope.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/Q25W1ZCYl1M" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>You can see from the video that the data of Gyroscope is pretty smooth ~</p>
                            <p>Now we compare the pitch data from Accelerometer and Gyroscope. We can get pitch output from Accelerometer and Gyroscope simultaniously, and the results are as follows:</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/S5qKoHjtD2I" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>We can see that the pitch noise obtained from the accelerometer is very large, while the pitch obtained from the gyroscope is very smooth. However, the data obtained from the gyroscope has accumulated errors; when the board is stationary, the pitch calculated by the gyroscope still changes.</p>
                            <!-- change sampling frequency -->
                            <p>Additionally, we change the sampling frequency to see how it affects the accuracy of gyroscope data.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/lowerf.png" alt="">
                            <p>After lowering the frequency, the accumulated error in gyroscope data becomes more pronounced, while the accelerometer data remains unaffected. This indicates that the sampling frequency is crucial for the accuracy of gyroscope data.</p>
                            <div class="subheading mb-3">Complimentary Filter</div>
                            <!-- conbine -->
                            <p>To address the issue of high noise in accelerometer readings without cumulative errors, and low noise but cumulative errors in gyro readings, we merge the readings of both sensors using the following filter:</p>
                            <img class="img-fluid img-centered" src="assets/img/lab2/combineequation.png" alt="">
                            <p>Artemis implementation:</p>
                            <code>
                                pitch_g_delta = myICM.gyrX()*dt; <br>
                                roll_g_delta = myICM.gyrY()*dt; <br>
                                
                                pitch_a = atan2(myICM.accY(),myICM.accZ())*180/M_PI; <br>
                                roll_a  = atan2(myICM.accX(),myICM.accZ())*180/M_PI; <br>

                                pitch = (pitch + pitch_g_delta)*(1-alpha) + pitch_a * alpha; <br>
                                roll = (roll + roll_g_delta)*(1-alpha) + roll_a * alpha; <br>
                                yaw = yaw + myICM.gyrZ()*dt; <br>
                            </code>
                            <!-- alpha -->
                            <p>Here alpha is set to 0.1 based on practical experience. Now the pitch and roll we get is both accurate and stable.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/iFTiorOolmY" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div>


                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task4: Sample Data</h3>
                            <div class="subheading mb-3">Speed Up</div>
                            <p>Just like the GET TEMP READINGS task in lab2, we store the timestamps and IMU data in an array and send to PC once the Artemis board received a command.</p>
                            <p>The data we received on PC looks like this.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab2/speedup.png" alt="">
                            <p>We can see that the sample rate is around 3.15ms, which is around 317Hz. I've written in the code that if the IMU data is not ready, 0 will be filled into the array. But we didn't receive 0s in the array which indicates that the main loop on the Artemis run slower than the IMU produces new values.</p>

                            <div class="subheading mb-3">Data Storage</div>
                            <p>Firstly, I think it makes more sense to store Timestamp, Accelerometer, Gyroscope, and ToF separately. It would be easier to locate data by index in separate arrays than a single combined one. Also, it would be more convenient to use separate arrays if we are dealing with different data types.</p>
                            <p>Secondly, data types. I would use integers (2or4 bytes) for timestamp, floats (4 bytes) for Accelerometer (X,Y,Z) and Gyroscope (X,Y,Z) data to save storage.</p>
                            <p>Lastly, the above mentioned sample rate would be too large to deal with. If we choose to record at a sample rate of around 50Hz, we will use 1400 bytes per second. With the 384kB internal storage we can store around 274s.</p>
                            <div class="subheading mb-3">5S Time-stamped IMU Data</div>
                            <p>Here is the video to demonstrate the ability to record 5 second of data.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/VREiisrrA1g" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>One can tell from the timestamps that the time span is longer than 5s and we have recorded 255 sample points.</p>
                        </div>
                    </div>


                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task5: Record a Stunt</h3>                                                        
                            <p>To get a sense of how RC car moves, we use the remote control to drive the car.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/qss7fXpiP84" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>The RC car moves forward, backward, and turns at very high speeds. During remote control, each step is large, and fine adjustments cannot be made. It seems that the car's speed only have one gear, leaving a lot of room for optimization.</p>
                        </div>
                    </div>

                </div>
            </section>
            <hr class="m-0" />

            <!-- LAB 3 -->
            <section class="resume-section" id="lab3">
                <div class="resume-section-content">
                    <h2 class="mb-5">LAB3: Time of Flight Sensors</h2>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Lab Objective</h3>
                            <p>Adding Time of Flight (ToF) sensors to a robot can greatly enhance its ability to navigate and avoid obstacles effectively. ToF sensors work by emitting infrared light pulses and measuring the time it takes for the light to bounce back, allowing the robot to determine the distance to objects in its path.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Prelab</h3>
                            <div class="subheading mb-3">1. I2C sensor address</div>
                            <p>From the datasheet, the address of two ToF sensors are both 0x52. In following tasks we will find that it shows differently and we also need the address the issue of communicating with two identical sensors (originally with same address).</p>

                            <div class="subheading mb-3">2. Using 2 ToF Sensors</div>
                            <p>To use 2 ToF Sensors, the idea is to shutdown one of them using the XSHUT pin, change the address of the one which is online, then power up the other one.</p>
                            
                            <div class="subheading mb-3">3. Wiring diagram</div>
                            <p>For QWIIC wires, Yellow-SCL, Blue-SDA, Red-VIN, Black-GND.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab3/color.jpg" alt="">
                            <p>Here is the design of wiring. We use two long cables to connect ToF sensors and the short ones to connect the breakout board and the IMU. Because IMU will be put inside the robot while ToF sensors should be put outside.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab3/wiring.jpg" alt="">

                            <div class="subheading mb-3">4. Placement of sensors in future labs</div>
                            <p>I will put one sensor in front of the robot to detect incoming obstacles, the other one on one side of the robot (probably right side since cars are driving on the right side:)). It's worth mentioning that having two sensors set in front of the robot may interfere with each other because they will receive signals from each other.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task1: QWIIC breakout board connection</h3>
                            <p>First we use a JST connector and a battery to power up the Artemis board and install the SparkFun VL53L1X 4m laser distance sensor library.</p>
                            <p>Then we use solder a QWIIC cable to our ToF sensors and connect them to the QWIIC breakout board.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab3/connection.jpg" alt="">
                            <p>The additional white wire will be discussed in following tasks.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task2: Read I2C address</h3>
                            <p>From the screenshot we can see that the address is 0x29, which is different from the datasheet (0x52).</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab3/address.png" alt="">
                            <p>Through further research, we can discover that 0x29(00101001) is one bit moving left from 0x52(01010010), that's probably because the rightmost bit of 0x52 is used to indicate read/write status and it was taken off on Artemis side.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task3: Sensor data in different modes</h3>
                            <p>The ToF sensors we use have two ranging mode, namely setDistanceModeShort() and setDistanceModeLong(), ranging 1.3m and 4m respectly by default.</p>
                            <p>To select one the these two modes, I did a experiment to test there accuracy and repeatability.</p>
                            <p>The video below shows my experiment setting:</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/DcPx9Uq8xpg" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>I chose 150, 300, 450, ..., 1200 eight sample points, got the average reading from both modes.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab3/modediff.png" alt="">
                            <p>You can see that both modes have pretty accurate readings. Given that our robot can move in a fast speed (as shown in lab2), choosing the long range mode might be more usefu.</p>
                            <p>I also use matplotlib to draw the picture as shown below.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab3/modediff_python.png" alt="">
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task4: 2 ToF sensors</h3>
                            <p>In prelab we mentioned the issue of two ToF sensor sharing one default address. By checking the head file of ToF sensor library, we can see many functions that come in handy.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab3/tofclass.png" alt="">
                            <p>We will use setI2CAddress() function to address the issue.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab3/twotof.png" alt="">
                            <p>After changing the address of one of the sensors, we can call them separately just like in examples. One can see in the serial output that two sensors are working in parallel.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab3/wiring.jpg" alt="">
                            <p>To illustrate, the white wire was used to shutdown sensor1.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task5: Tof sensor speed</h3>
                            <p>Executing speed is critical in future labs. So we measure the time it takes for ToF sensors to receive ranging data.</p>
                            <p>In this task the board will print time information no matter it receives a new ToF data or not.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab3/speedupcode.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab3/speedup.png" alt="">
                            <p>Results show that it takes around 3ms for the board to run a loop, 60ms for the ToF sensors to receive a new data, and around 8ms to process the ToF data. The limiting factor here is probably the time it takes for sensors to transmit signals.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task6: Time v Distance</h3>
                            <p>By combining previous labs the managed to use bluetooth to collect ToF data.</p>
                            <p>After collecting, I used matplotlib to draw the graph.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab3/plot_python.png" alt="">
                           
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">5000-level: Discussion on infrared transmission based sensors</h3>
                            <p>Two common sensors based on infrared transmission are Infrared (IR) Proximity Sensors and Infrared Distance Sensors:</p>
                            <p>1. Infrared Proximity Sensors</p>
                            <p>Functionality: IR proximity sensors emit infrared light and measure the reflection of this light to detect nearby objects. They work based on the principle that objects reflect infrared light differently depending on their surface properties and distance from the sensor.</p>
                            <p>Pros:<br>
                                Simple and cost-effective.<br>
                                Suitable for detecting the presence or absence of objects within a limited range.<br>
                                Fast response time, making them suitable for applications requiring quick detection.</p>
                            <p>Cons:<br>
                                Limited range and accuracy compared to other distance sensors.<br>
                                Susceptible to interference from ambient light sources, which can affect their reliability.<br>
                                May have difficulty distinguishing between objects of similar reflectivity.</p>
                            <p>2. Infrared Distance Sensors</p>
                            <p>Functionality: Infrared distance sensors also emit infrared light, but they measure the time it takes for the emitted light to bounce back (Time of Flight principle) to calculate the distance to an object. They typically use methods like triangulation or phase-shift measurement to determine distance accurately.</p>
                            <p>Pros:<br>
                                Offer greater accuracy and range compared to IR proximity sensors.<br>
                                Can provide precise distance measurements over longer distances.<br>
                                Less affected by ambient light interference due to sophisticated modulation and signal processing techniques.</p>
                            <p>Cons:<br>
                                Generally more complex and expensive than IR proximity sensors.<br>
                                May require calibration and adjustment for optimal performance.<br>
                                Sensitive to environmental factors like temperature and humidity, which can affect their accuracy.</p>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">5000-level: Sensitivity of sensors to colors and textures</h3>
                            <p>Time of Flight sensors are generally less sensitive to variations in color and texture. As the picture shows I tested several material but didn't find much difference. However, transparent or translucent objects may absorb or scatter infrared light differently, impacting the accuracy of distance measurements obtained by ToF sensors.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab3/texture.jpg" alt="">
                        </div>
                    </div>

                </div>
            </section>
            <hr class="m-0" />


            <!-- LAB 4 -->
            <section class="resume-section" id="lab4">
                <div class="resume-section-content">
                    <h2 class="mb-5">LAB4: Motors and Open Loop Control</h2>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Lab Objective</h3>
                            <p>In this lab we'll replace the chip inside our racing car with the Artemis board, solder the motor drivers with the board and two motors separately. By the end of the lab, we'll be able to drive the car via pre-programmed codes or bluetooth connection.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Prelab</h3>
                            <div class="subheading mb-3">1. Wiring Diagram</div>
                            <p>As shown in the following diagram, in order to get enough current we need, we use both of two channels on one driver chip to actuate one motor. Note that channels on different chips should not be used to drive one motor, because they will interfere with each other.</p>
                            <p>The two GND pins on a driver chip are essentially the same, so we only have to hook up one GND to the Artemis.</p>
                            <p>In terms of analogWrite PIN, I chose A14 and A15 for one driver, A2 and A3 for the other. We only have to make sure the pin we use has the ability to write analog signals. The sequence is not that important because we can easily switch the forward/backward pin in Arduino code.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab4/wiringdiagram.png" alt="">
                            <div class="subheading mb-3">2. Battery Discussion</div>
                            <p>We will use separate batteries for the Artemis board and the motor drivers for several reasons.</p>
                            <p>(1)Motors can generate electrical noise and voltage spikes, especially during sudden changes in speed or direction. This electrical noise can potentially interfere with the operation of the Artemis microcontroller, causing malfunctions or erratic behavior. By using separate batteries, we can isolate the power supplies for the Artemis and the motors, reducing the likelihood of interference.</p>
                            <p>(2)Motors often require higher voltages and currents than microcontrollers. Using separate batteries allows us to choose a battery with appropriate voltage and current ratings for the motors without worrying about compatibility issues with the Artemis microcontroller.</p>
                            <p>(3)Separating the power supplies can also improve the overall efficiency and performance of the system. Motors drawing large currents won't cause voltage droops in the power supply for the microcontroller, ensuring consistent and reliable operation of both the Artemis and the motors.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task1: Test analogWrite</h3>
                            <p>Before connecting the driver to the real motors, we first test the analogWrite function of the combination of Artemis and motor driver.</p>
                            <div class="subheading mb-3">Test Setup</div>
                            <p>As you can see in the picture, I soldered up one of the motor driver with the Artemis board, and use power supply the power up the driver.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab4/driver_test.jpg" alt="">
                            <div class="subheading mb-3">Power Supply Discussion</div>
                            <p>For the setting of the power supply, I set it to 3.7V to keep up with the battery supply.</p>
                            <div class="subheading mb-3">analogWrite code</div>
                            <p>Before setup, we define the output pins for motor control.</p>
                            <code>
                                #define MOTOR_L_FORWARD 15 <br>
                                #define MOTOR_L_BACKWARD 14 <br>                                
                            </code>
                            <p>In setup, we set the resolution of analogWrite output. By default it's 8 bits, so we can skip this line. We need to explicitly set it if we want 1-16 bits other than 8.</p>
                            <code>
                                analogWriteResolution(8); <br>                          
                            </code>
                            <p>In the loop, we write one pin to high and the other to 0.</p>
                            <code>
                                analogWrite(MOTOR_L_FORWARD, 127);    // 50% duty cycle w/ 8-bit resolution <br>
                                analogWrite(MOTOR_L_BACKWARD, 0); <br>
                                delay(1);    <br>                            
                            </code>
                            <p>The output looks like this.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab4/square50.jpg" alt="">
                            <p>I also changed this value to 63(25%) and 191(75%).</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab4/square25.jpg" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab4/square75.jpg" alt="">
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task2: Take apart!</h3>
                            <p>Now it's the time for us to take our RC cars apart. After removing the shell, we'll see two motors and the control PCB comes with it. Carefully cut the wires and get rid of the PCB and LEDs. Now we have two moters and one battery connector all with original wires.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab4/take_apart.jpg" alt="">
                            <p>Note the each motor control two wheels on one side, which means it is a differential drive robot.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task3: Spinning Test</h3>
                            <p>After removing the PCB and LEDs, we hook up the drive and the motor to get a physical test of analogWrite.</p>
                            <div class="subheading mb-3">Single side test</div>
                            <p>As you can see in this video, the motor first spin forward for 4s, rest for 1s, and then spin backward for another 4s.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/AfZmbdSbTwE" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <div class="subheading mb-3">All-wheel-drive with Battery Supply</div>
                            <p>Repeat the above process for the other motor and driver, and hook up two drivers with the battery connector. Then we can let the car run by it self.</p>
                            <p>It's worth mentioning that I forgot to pass the battery connector through the hole in the battery box before soldering it, which led to the need to start over. Thanks to TA Julian for providing me with another battery connector, allowing me to cut the cable in an easily accessible part, thus avoiding the need to re-solder the part connected to the driver.</p>
                            <p>Now you can see both wheels spinning, and the car running on the ground.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/NBXfgtG5XlE" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>Notice that due to the different spin rate of these two motors, the car didn't run in a vary straight line. We'll do a calibration to fix this problem in the flollowing part.</p>
                            <p>The picture of all the components secured.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab4/secured.png" alt="">
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task4: Static Friction</h3>
                            <p>Due to friction inside the motor and gear box, the motor cannot be actuated at PWM values just above 0. So it is important for us to measure this lower limit.</p>
                            <p>To make my experiment easier, I add a "GO_STRAIGHT" command on top of the bluetooth control from previous labs. It can read in a integer from Jupyter lab and implement this speed for analogWrite.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab4/gostraight.png" alt="">
                            <p>By increasing from 0, I found out that the lower limit is around 36-38 (out of 0-255).</p>
                            <p>For on-axis turns, the threshold is much higher at around 175.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task5: Calibration</h3>
                            <p>In the end of task3, we noticed the different spin rate of two motors. By implementing a calibration factor on one of the driver we can address this problem.</p>
                            <p>By trail and error, I found out that the calibration factor is actually around 1. Other factors such as the alignment of the vehicle's front end with the straight line, and the installation of tires, have a greater impact on straight-line driving.</p>
                            <p>Here is a video to demonstrate that my car can drive in a fairly straight line for more than 2m.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/WQJVCqYWy_k" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>Actually, I believe that slight deviations to the left or right during straight-line driving have limited impact on actual autonomous driving. These errors can be promptly corrected under the closed-loop control of distance sensors.</p>
                        </div>
                    </div> 

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task6: Open Loop Demonstration</h3>
                            <p>Lastly, we wrap up with a open loop control demo, including straight lines, back up and turns.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/rciE1oPk6DI" frameborder="0" allowfullscreen></iframe>
                            </div>
                        </div>
                    </div> 

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">5000-level: AnalogWrite Frequency</h3>
                            <p>The analogWrite function in Arduino generates a PWM signal with a frequency typically around 490 Hz for most Arduino boards. This frequency is generally suitable for driving many types of motors, including DC motors, servos, and some types of stepper motors.</p>
                            <p>However, for certain applications or specific motor types, a faster PWM frequency may offer benefits</p>
                            <p>1. Reduced Audible Noise: Motors driven at higher PWM frequencies tend to produce less audible noise, which can be advantageous in applications where noise is a concern, such as in audio equipment or robotics used in quiet environments.</p>
                            <p>2. Higher Control Resolution: Faster PWM signals allow for finer control of motor speed and position, as the shorter pulse widths provide more discrete steps in the motor's operation. This can be beneficial for applications requiring precise control, such as CNC machines.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">5000-level: Dynamic Friction</h3>
                            <p>Once the car is in motion, the lower limit of PWM value should be lower than what was been found in task4, where the car starts from static.</p>
                            <p>To measure the dynamic lower limit, I modified previous code as below.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab4/gostraight_dy.png" alt="">
                            <p>The car is given an initial speed which is higher than the lower limit we found in task4. Then we change the speed to what we want to test.</p>
                            <p>I found out that the dynamic lower limit is around 32 (out of 0-255) in this test below.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/frSypnccBrk" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>One can tell whether the motors are running by listening to the sound. If you can hear the sound from the motors but the car stopped moving, the current value is below the lower limit.</p>
                            <p>When the car is moving at its slowest speed, it takes nearly no time for it to stop.</p>
                        </div>
                    </div>

                </div>
            </section>
            <hr class="m-0" />

            <!-- LAB 5 -->
            <section class="resume-section" id="lab5">
                <div class="resume-section-content">
                    <h2 class="mb-5">LAB5: Linear PID control and Linear interpolation</h2>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Lab Objective</h3>
                            <p>This lab is part of a series of labs (5-8) on PID control, sensor fusion, and stunts. This week we will do position control. Specifically, drive the robot towards a wall, then stop when it is 304mm away from the wall using feedback from the time of flight sensor.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab5/car.jpg" alt="">
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Prelab</h3>
                            <div class="subheading mb-3">Data Transfer</div>
                            <p>Before implementing the PID controller, it's very important for us to setup a data transfer system via bluetooth.</p>
                            <p>To begin with, I modified the notification handler from previous labs to receive data.</p>
                            <code>
                                def notification_handler(uuid, byte_array): <br>
                                    global times, tof1, pwm<br>
                                    time, tof11, pwm1 = ble.bytearray_to_string(byte_array)[:].split()<br>
                                    times.append(int(time))<br>
                                    tof1.append(int(tof11))<br>
                                    pwm.append(int(pwm1))<br>
                                ble.start_notify(ble.uuid['RX_STRING'], notification_handler);                   <br>  
                            </code>
                            <p>To make my debug and tuning easier, I wrote another two commands to communicate with the Artemis.</p>
                            <p>First is a PID_SWITCH command. By sending this</p>
                            <code>
                                ble.send_command(CMD.PID_SWITCH, "1")        <br>  
                            </code>
                            <p>I can start the PID controll, wheras "0" to stop. Initially, it's set to "0" so that I can place the car easily.</p>
                            <p>Next is a SET_PID_GAIN command. By sending this</p>
                            <code>
                                ble.send_command(CMD.SET_PID_GAIN, "0.15|0.01|0")    <br>  
                            </code>
                            <p>I can set Kp=0.15, Ki=0.01, Kd=0, which makes my tuning much easier.</p>
                            <p>On Arduino side it looks like this:</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab5/PIDGain.png" alt="">
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task1: PID Controller</h3>
                            <p>We now can dive into the controller itself. We have talked about PID Controller in class. The basic equation is as follow: </p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab5/PID.png" alt="">
                            <p>Proportional (P) Control: This term produces an output that is proportional to the current error signal, which is the difference between the desired setpoint and the actual value of the system being controlled.</p>
                            <p>Integral (I) Control: This term integrates the error signal over time, which helps to eliminate steady-state errors by continuously adjusting the output based on the accumulated error. The integral term helps to reduce any long-term deviations from the setpoint.</p>
                            <p>Derivative (D) Control: This term considers the rate of change of the error signal. It acts to dampen the system's response by anticipating future trends in the error signal.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab5/graph.png" alt="">
                            <div class="subheading mb-3">PI Controller</div>
                            <p>In my experiment, I chose to use a PI controller for several reasons.</p>
                            <p>1. Steady-State Accuracy: The integral term in a PI controller continuously adjusts the control signal to eliminate steady-state errors. This means that the system output eventually reaches and maintains the desired setpoint accurately, even in the presence of disturbances or uncertainties.</p>
                            <p>2. Less Sensitivity to Noise: In some cases, the derivative term in a PID controller can amplify noise in the system, leading to undesirable control action. By excluding the derivative term, PI controllers are less sensitive to noise, which can be beneficial in noisy environments or systems with high sensor noise.</p>
                            <p>3. Reduced Complexity: Compared to PID controllers, PI controllers have simpler dynamics and fewer tuning parameters. This reduced complexity can lead to easier implementation, maintenance, and troubleshooting in control system applications.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task2: Range/Sampling time discussion</h3>
                            <p>The sampling frequency of the ToF sensor is relatively low, around 40ms in my case. And I'm alredy using the shortmode. It would be lower if I chose the longmode.</p>
                            <p>So I kept using this mode and came up with two strategies which we will further discuss. Namely, controll with previous information and with extrapolation data.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task3: implementation</h3>
                            <p>The implementation of PID controll is relatively simple. On Artemis it looks like this:</p>
                            <code>
                                while (!distanceSensor1.checkForDataReady()){delay(1);}<br> 
                                time_last = time_current;<br> 
                                time_current = (int)millis();<br> 
                                int dt = time_current - time_last;<br> 
                                int distance1 = distanceSensor1.getDistance();<br> 
                                distanceSensor1.stopRanging();<br> 
                                distanceSensor1.clearInterrupt();<br> 
                                distanceSensor1.startRanging();<br> 
                                error_current = distance1 - goal;<br> 
                                error_sum = error_sum + int(error_current*dt/1000);<br> 
                                int p = kp * error_current;<br> 
                                int i = ki * error_sum;<br> 
                                float d = kd * (error_current - error_previous)/dt;<br> 
                                int speed = p+i+int(d);           <br>  
                            </code>
                            <p>In each loop we wait for ToF data ready, then get the time information and new ToF data. After that we compute three terms separately and add them together.</p>
                            <p>The output "speed" will go to my actuate function and translate to PWM value. In my actuate function I wrote a mapping between demanding speed and PWM value, particularly considered the dead band tested in last lab.</p>
                            <p>Here is a video to demonstrate the basic feedback controll.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/dUw9IGN3KbY" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>One thing I noticed during my debugging is that I have to set my integration term to 0 when I stop my PID controll. Otherwise this term will get weird.</p>
                            <div class="subheading mb-3">Reaching Task Goal</div>
                            <p>I tuned the PID gains according to the strategy we have talked about in class. And get the following combination and result.</p>
                            <code>
                                ble.send_command(CMD.SET_PID_GAIN, "0.1|0.01|0")    <br>  
                            </code>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/R5ekaT-FFE4" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>You can see that the car bump into the carton no matter how I tune this. This is probably caused by the low controll frequency, which leaves up space to add extrapolation.</p>
                            <p>The sensor readings and controll values over time are recorded as below.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab5/tof-time2.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab5/pwm-time2.png" alt="">
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task4: Extrapolation</h3>
                            <p>Although the update frequency of ToF sensors are relatively low, we can increase our controll rate by predicting position information. By implementing a linear prediction on previous two data, we can get a more accurate new data rather than using the previous one.</p>
                            <p>The pseudocode for extrapolation is as follow. We have to memorize two previous data points (timestamp+reading) to do linear extrapolation.</p>
                            <code>
                                if newdata: <br> 
                                    updata previous data point<br>
                                    controll according to new data <br>
                                else:<br>
                                    compute linear extrapolation<br>
                                    controll according to extrapolation data <br>
                            </code>
                            <p>One thing I noticed during testing is that, we should avoid doing extrapolation on extrapolation data. I tried to record both raw data and extrapolation data, and do extrapolation on previous two data points no matter they are raw or extrapolated. But it turns out the data quality get even worse. Because the update rate of extrapolation is even higher than raw, tiny error in sensor reading will accumulate during extrapolation. So it's important for us to restrict extrapolation on raw data.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">(5000)Task5: Wind-up and discussion</h3>
                            <p>Windup happens when the system's output saturates or reaches its limits, but the integral action continues to accumulate error. This can lead to overshooting, instability, or prolonged settling time when the output eventually returns within the allowable range.</p>
                            <p>To address this problem, the simple way I use is just clamp the integral term to a certain range, for example below 100. This will address this issue while keep the ability to reach steady state.</p>
                            <p>After implementing Extrapolation and addressing Wind-up, we can get a better controll outcome.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/xnmLqkr26xo" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>The sensor readings and controll values over time are recorded as below.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab5/tof-time1.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab5/pwm-time1.png" alt="">
                        </div>
                    </div> 


                </div>
            </section>
            <hr class="m-0" />

            <!-- LAB 6 -->
            <section class="resume-section" id="lab6">
                <div class="resume-section-content">
                    <h2 class="mb-5">LAB6: Orientation Control</h2>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Lab Objective</h3>
                            <p>This lab is part of a series of labs (5-8) on PID control, sensor fusion, and stunts. Quite parallel to the last lab, in this one we will controll the yaw of our robots using the IMU.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab5/car.jpg" alt="">
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Prelab</h3>
                            <div class="subheading mb-3">Data Transfer</div>
                            <p>Before implementing the PID controller, it's very important for us to setup a data transfer system via bluetooth.</p>
                            <p>I did slight chances to last lab's code to transfer data.</p>
                            <script src="https://gist.github.com/dyjin2/f46272cd254a329fb90880dcf6ae2c50.js"></script>
                            <p>To make my debug and tuning easier, I wrote another two commands to communicate with the Artemis.</p>
                            <p>First is a PID_TURN command. By sending this</p>
                            <code>
                                ble.send_command(CMD.PID_TURN, "1|90")        <br>  
                            </code>
                            <p>First item "1" start the PID controll, wheras "0" to stop. Initially, it's set to "0" so that I can place the car easily.</p>
                            <p>Second item "90" is used to set the turning angel measured by degree according to Right-hand thread.</p>
                            <p>Next is a SET_PID_GAIN command similar to last lab. By sending this</p>
                            <code>
                                ble.send_command(CMD.SET_PID_GAIN, "0.15|0.01|0.1")    <br>  
                            </code>
                            <p>I can set Kp=0.15, Ki=0.01, Kd=0.1, which makes my tuning much easier.</p>
                            <p>On Arduino side it looks like this:</p>
                            <script src="https://gist.github.com/dyjin2/17beaf079c1a6a6bf5f19484efe95797.js"></script>
                            <p>Furthermore, the command to send back turing data is also modified accordingly on Artemis.</p>
                            <script src="https://gist.github.com/dyjin2/01a2d558b2f61a2547098d9e67113f3e.js"></script>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task1: PID Controller</h3>
                            <p>We now can dive into the controller itself. We have talked about PID Controller in class. The basic equation is as follow: </p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab5/PID.png" alt="">
                            <p>Proportional (P) Control: This term produces an output that is proportional to the current error signal, which is the difference between the desired setpoint and the actual value of the system being controlled.</p>
                            <p>Integral (I) Control: This term integrates the error signal over time, which helps to eliminate steady-state errors by continuously adjusting the output based on the accumulated error. The integral term helps to reduce any long-term deviations from the setpoint.</p>
                            <p>Derivative (D) Control: This term considers the rate of change of the error signal. It acts to dampen the system's response by anticipating future trends in the error signal.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab5/graph.png" alt="">
                            <div class="subheading mb-3">PID Controller</div>
                            <p>In this lab, I chose to use a PID controller for several reasons.</p>
                            <p>1. Steady-State Accuracy: The integral term in a PI controller continuously adjusts the control signal to eliminate steady-state errors. This means that the system output eventually reaches and maintains the desired setpoint accurately, even in the presence of disturbances or uncertainties.</p>
                            <p>2. Damping Oscillations: The derivative action of a PID controller helps to dampen oscillations in the system's response, particularly during transient periods or when the system experiences sudden changes.</p>
                            <p>3. Preventing Integral Windup: The derivative term indirectly helps to prevent integral windup by limiting the integral action during rapid changes in the error signal.</p>
                            <p>4. Also the Gyroscope readings are relatively stable, which provides the foundation to implement derivative controll.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task2: Range/Sampling time discussion</h3>
                            <p>The sampling frequency of IMU is relatively high, beyond 300fps in my case. So I think there's no need to use extrapolation and I can simply put the control loop inside myICM_dataReady function.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task3: implementation</h3>
                            <p>Here I first implement a turning controll without bluetooth</p>
                            <p>The PID controll code on Artemis looks like this:</p>
                            <script src="https://gist.github.com/dyjin2/9f372cea8181dd6a0387cae0ad53e9de.js"></script>
                            <p>In each loop we wait for IMU data ready, then get the time information and new Gyroscope data. After that we compute three terms separately and add them together.</p>
                            <p>In the end, we have to write a function to translate the PID result to wheel controll. For turning it's as follow:</p>
                            <script src="https://gist.github.com/dyjin2/8931d65dd2c2021f0de03d5a7e376f81.js"></script>
                            <p>Here you can see that there is a mapping between PID output and analogWrite range. The dead band for turning is relatively larger than moving straightward.</p>
                            <p>Here is a video to demonstrate the basic PID turning controll.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/KCO55fkkSpo" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>In this video the target angle is set to 90. As you can see, the car tried to maintain at 90 degrees when I tried to interrupt it.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task4: Bluetooth and data collection</h3>
                            <p>After that I combined Bluetooth controll.</p>
                            <p>I tuned the PID gains according to the strategy we have talked about in class. First get the following combination without derivative term.</p>
                            <code>
                                ble.send_command(CMD.SET_PID_GAIN, "0.3|0.01|0")    <br>  
                            </code>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/0TlBxTi2wk4" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab6/thetatime-overshoot.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab6/pwmtime-overshoot.png" alt="">
                            <p>You can see that there is a large overshoot. We can add a derivative term to prevent this.</p>
                            <code>
                                ble.send_command(CMD.SET_PID_GAIN, "0.4|0.01|0.1")    <br>  
                            </code>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/K-WE8V_OYYc" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab6/thetatime.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab6/pwmtime.png" alt="">
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">(5000)Task5: Wind-up and discussion</h3>
                            <p>Windup happens when the system's output saturates or reaches its limits, but the integral action continues to accumulate error. This can lead to overshooting, instability, or prolonged settling time when the output eventually returns within the allowable range.</p>
                            <p>To address this problem, the simple way I use is just clamp the integral term to a certain range, for example below 30. This will address this issue while keep the ability to reach steady state.</p>
                            <p>After addressing Wind-up, I did more test on 30 degrees and 180 degrees to show the car's ability to make turns.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/DOCUDk4pKtI" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/0V8kHuGMhws" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <div class="subheading mb-3">Further Discussion</div>
                            <p>To control the orientation while the robot is driving forward or backward, we can simply add up the controll command for both straightward and turning, with some calibration factors to balance the contribution of both motions.</p>
                        </div>
                    </div> 


                </div>
            </section>
            <hr class="m-0" />

            <!-- LAB 7 -->
            <section class="resume-section" id="lab7">
                <div class="resume-section-content">
                    <h2 class="mb-5">LAB7: Kalman Filter</h2>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Lab Objective</h3>
                            <p>In this lab we will implement a Kalman Filter, which can supplement our slowly sampled ToF values to make our controll smoother.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task1: Estimate drag and momentum</h3>
                            <p>Before implementing the Kalman Filter, we first have to model the state space of our robot. Specifically, estimate the drag and momentum terms for our A and B matrices.</p>
                            <p>To get these parameters, we will drive our robot towards a wall from stationary position, record the process of acceleration and steady speed, and analyze the speed graph.</p>
                            <p>Note that the floor conditions should remain similar between the experiment from which we model these parameters and the experiment we want to implement KF.</p>
                            <P>Because I did my position control on a carpet, I went back to the same place to model my state space.</P>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://www.youtube.com/embed/OhSTQ3oz_lw" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>The motor output is set at PWM 220 out of 255. I recorded data from ToF sensor and time stamps, then calculated the speed.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab7/tof-time.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab7/speed-time.png" alt="">
                            <p>From the speed graph, we can read the steady state speed, the speed at 90% risetime, and 90% rise time.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab7/plot-time.png" alt="">
                            <p>The steady state speed is around 2.8m/s, 90% rise time is around 0.65s.</p>
                            <p>Using these data, we can calculate drag and momentum from these two equations:</p>
                            <img class="img-fluid img-centered" src="assets/img/lab7/two_equation.png" alt="">
                            <p>And get d = 220/2800 = 0.0786, m = -d*650/ln(0.1) = 0.0341.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task2: Implement Kalman Filter</h3>
                            <div class="subheading mb-3">Initialize KF</div>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab7/steadystate_equation.png" alt="">
                            <p>From first task we get our A, B, C matrices, and we will discretize them in the following code</p>
                            <script src="https://gist.github.com/dyjin2/d7a5ae94fc717f1e1235e2374360bc82.js"></script>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab7/parameters.png" alt="">
                            <p>Then we initialize process and sensor noise covariance matrices</p>
                            <script src="https://gist.github.com/dyjin2/47540f7b6807d051c80d053c007a719f.js"></script>
                            <p>Finally we can initialize the state</p>
                            <script src="https://gist.github.com/dyjin2/b1656009b882ad4d03323c6486ad0864.js"></script>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab7/kf_equation.png" alt="">
                            <p>We already have the KF function from class, simply call this function in every iteration as below.</p>
                            <script src="https://gist.github.com/dyjin2/a5d83b9a2631225b2603b8395e85e04c.js"></script>
                            <p>Now we get the output from Kalman Filter.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab7/kf-result.png" alt="">
                            <p>You can see that the trend of KF line is highly consistent with the observed ToF results, but it's smoother.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">(5000)Task3: Faster Frequency</h3>
                            <p>Even if we don't have a ToF reading, we can still get a update prediction from the Kalman Filter.</p>
                            <p>In the following example, I insert a timestamp between each ToF reading to enhance prediction frequency. </p>
                            <script src="https://gist.github.com/dyjin2/b1fe20bb8f76f87af4a6d5b7c6d70c13.js"></script>
                            <script src="https://gist.github.com/dyjin2/fd0732f680fc78c3f83c1abb1447a4ae.js"></script>
                            <p>For each ToF&PWM reading, I would update twice, one with ToF, the other without ToF. Thee output shows like this:</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab7/kf-hf-result.png" alt="">
                        </div>
                    </div> 


                </div>
            </section>
            <hr class="m-0" />

            <!-- LAB 8 -->
            <section class="resume-section" id="lab8">
                <div class="resume-section-content">
                    <h2 class="mb-5">LAB8: Stunts!</h2>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Lab Objective</h3>
                            <p>In this lab we will combine everything we've done up till now to do fast stunts. We can choose between position control (flip) and orientation control (drift).</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task: Drift!!!</h3>
                            <p>In this lab I chose to do a drift, although technically it should not be called one. </p>
                            <p>It's basically a combination of driving towards the wall, do a 180 degree turn, and speed up again. However, because our car has built up momentum during the first phase, the 180 degree turn would look like a drift.</P>
                            <p>I also added some tricks in the end to make it more like a drift.</p>
                            <p>Here I first show the result of my basic setting of position control:</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://youtube.com/embed/2SMTMtwfK4s?feature=share" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>To get this, I combined several modules from previous labs, and constructed this pseudocode structure:</p>
                            <script src="https://gist.github.com/dyjin2/0f6cdbbfe0b557e7ae051e1ef2068fb2.js"></script>
                            <p>In order to get sensors'data, the robot first collects and records them in each loop. Then it will select the task according to the task flags, and complete each task in sequence.</p>
                            <p>After completing all the tasks, I will collect data via bluetooth.</p>
                            <p>The sensors' data of this demo is shown below.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab8/gyro-time.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab8/tof-time.png" alt="">
                            <p>Note that the Gyro data is updated in each loop, but the ToF data is not available in each loop. Simply waiting for ToF data in every loop will decrease my processing frequency, what I did is checking the availability of ToF data in every loop and record it when it's available. Thus you can see that the graph of ToF data looks like spikes cause when there's no data it shows 0.</p>
                            <p>However, this basic setting is not very stable as the spin rate is too fast for the IMU to record, failure happens frequently as this one.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://youtube.com/embed/1SPzCUgZ03o" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>Then I tried to lower the spinning speed a little bit. Furthermore, I figured out that to make it more like a drift, I don't have to use zero point turn, which means if I want the car to make a left drift, I can set the right wheels spinning forward and the left wheels spinning backward but not necessarily at same speed. I can use left wheels mainly as a brake and use right ones to spin. This will result in a drift-like turn.</p>
                            <p>The following video shows the result.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://youtube.com/embed/sU7Yb6wuhxU?feature=share" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>In order to showcase I can reproduce this I did two more recordings.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://youtube.com/embed/cVUeXQ8RQkg?feature=share" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://youtube.com/embed/pIg-yrhTfFc" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <P>That's it for my drift stunt!</P>
                        </div>
                    </div>

                </div>
            </section>
            <hr class="m-0" />


            <!-- LAB 9 -->
            <section class="resume-section" id="lab9">
                <div class="resume-section-content">
                    <h2 class="mb-5">LAB9: Mapping</h2>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Lab Objective</h3>
                            <p>In this lab we will map an arena. We will first use the robot to scan the terrain at five positions using ToF sensors. Then complete the map construction through a series of transformations.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Part1: Orientation control</h3>
                            <p>The manual provides us with three methods for controlling angles, namely open loop control, orientation control, and angular speed control.</p>
                            <p>I chose to use orientation control which I can modify from previous labs. I set the angular control goal to 20 degree, which ideally would lead to 18 sample points per round.</p>
                            <p>Because we don't have the ground truth for angular value (the angle is calculated from integration), I didn't run the angular control throughout the spin. Instead, I reset the control after each 20 degree turn.</p> 
                            <p>Because the external environment is essentially unchanged, I believe this control strategy makes each rotation more independent and controllable.</p>
                            <p>Here is a sample video of the orientation control:</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://youtube.com/embed/-SZ4pfU7M9E" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>I initially planned to test the actual rotation results and adjust the angle for each rotation based on the actual number of rotations in one cycle. However, I found that this was not necessary, as the control of the rotations is already quite accurate.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Part2: Execute the Scan</h3>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab9/pic.jpg" alt="">
                            <p>Once the rotation control is determined, I can specify my scanning strategy. Below is the pseudocode for the scanning process.</p>
                            <script src="https://gist.github.com/dyjin2/d3a1bd7985113c9f1cb4985547e2c168.js"></script>
                            <p>By using notification_handler from previous labs, I got the ToF readings from the robot. Then I sanity checked the data by plotting them on a polar coordinate map:</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab9/code1.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab9/sample1_org.png" alt="">
                            <p>The result is quite good in my opinion. Note that the drawn line overlaps with itself in some parts, because I collected data points exceeding 360 degrees. This overlap indicates the reproducibility of the results.</p>
                            <p>Next, we need to transform the data into Cartesian coordinates for ease of merging.</p> 
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab9/convert.png" alt="">
                            <p>In addition to this, we should also recall that the ToF sensor is not located on the origin of the rotation. For simplicity we assume that the robot is rotating in place, then we need to measure the position from the sensors to the center of the robot as compensation for the readings.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab9/compensate.jpg" alt="">
                            <p>The compensation is set to 70mm. Furthermore, I also corrected the initial angle of the robot. </p>
                            <p>Finally, I got the map in Cartesian coordinates:</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab9/plot1.png" alt="">
                            <p>Through the same process, I obtained maps for other four sampling points, here are 4 polar maps for sanity check.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab9/polar2.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab9/polar3.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab9/polar4.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab9/polar5.png" alt="">
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Part3: Merge the Maps</h3>
                            <p>To begin with, the separate maps I've shown are in sequence inside this picture.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab9/sequence.jpg" alt="">
                            <p>In order to merge these maps, we first need to determine the origin of one of the maps as the origin for the fused map. Then, we calculate the transformation matrices for the other four maps in order to merge them.</p>
                            <p>The second point is chosen to be the origin of the merged map. Because I maintained a consistent initial orientation of the robot during the scanning process, there is no need for rotation between these five maps, only translation.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab9/matrix.png" alt="">
                            <p>The rotation matrix here should be an identity matrix, and the z term in translation is also 0.</p>
                            <p>We only need to count the number of tiles(1ft/tile) between each sampling point to obtain the corresponding x and y terms.</p>
                            <p>After translation, we got the merged map as following.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab9/merged.png" alt="">
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Part4: Line-Based Map</h3>
                            <p>Finally, we can add the "ground truth" of walls and obstacles on to the map, which makes it useful in following labs.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab9/lined_map.png" alt="">
                            <p>As you can see, the boundaries of the walls are clearly delineated, but there is a larger error in outlining the contours of the cardboard boxes in the arena. This may be due to differences in material of the obstacles, which could result in variations in the performance of the ToF sensors.</p>
                        </div>
                    </div>

                </div>
            </section>
            <hr class="m-0" />

            <!-- LAB 10 -->
            <section class="resume-section" id="lab10">
                <div class="resume-section-content">
                    <h2 class="mb-5">LAB10: Grid Localization using Bayes Filter</h2>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Lab Objective</h3>
                            <p>In this lab we will implement grid localization using Bayes filter. The manual for this experiment provided us with a great framework and a lot of useful information.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Function1: Compute Control</h3>
                            <p>Firstly, we have a function to compute the actual control result based on actual current pose and previous pose.</p>
                            <script src="https://gist.github.com/dyjin2/95c874a7f2033b7317cabf59562f61bb.js"></script>
                            <p>Note: Pay attention to the order of parameters in atan2 function.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Function2: Odom Motion Model</h3>
                            <p>After we obtain observations of the motion that occurred, we can use the Gaussian distribution to calculate the probability of this motion actually occurring under the given control command.</p>
                            <script src="https://gist.github.com/dyjin2/0b4834237e202065f5a70ad0dce1fe78.js"></script>
                            <p>I initially planurate.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Function3: Prediction Step</h3>
                            <p>Now we have enough information to update our predictions. For each grid, the new probability is the sum of the probabilities of moving to that grid from every other grid. This is why there are six nested loops in the code: the first three loops iterate over every grid, and the last three loops iterate over all grids from the previous time step to update the probabilities for this grid.</p>
                            <script src="https://gist.github.com/dyjin2/5c1e6120c0956ed25c43cd3d503b7d4a.js"></script>
                            <p>Note that we set a threshold here -- 0.0001. The threshold is used to skip calculations for some grids in order to improve computational speed. These grids with probabilities smaller than the threshold have minimal impact on the result.</p>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Function4: Sensor Model</h3>
                            <p>In this step, we will obtain the probabilities of observations from the sensor, which will be used in the final step to correct our predictions.</p>
                            <script src="https://gist.github.com/dyjin2/91d5fc2b2c79dc6205e69dcb412ec627.js"></script>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Function5: Update Step</h3>
                            <p>Finally, in the last step, we multiply the probabilities obtained from prediction by the probabilities from the sensor model, and then normalize them to obtain the final probability for each grid.</p>
                            <script src="https://gist.github.com/dyjin2/f84de3c4efd9cd4e99d6f74c951a841f.js"></script>
                        </div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Simulation Result</h3>
                            <p>The completion of the simulation is depicted below. The green path represents ground truth, the blue path represents the robot's belief, and the red path represents odometry measurements.</p>
                            <div class="embed-contaioner">
                                <iframe width="640" height="360" src="https://youtube.com/embed/cvXI-rzaiNI" frameborder="0" allowfullscreen></iframe>
                            </div>
                            <p>You can see that the results of Bayes Filter look much better than pure odometry measurements.</p>
                            <p>Here I selected some output data of several steps.</p>
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab10/exp1.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab10/exp5.png" alt="">
                            <img width="640" height="360" class="img-fluid img-centered" src="assets/img/lab10/exp14.png" alt="">
                        </div>
                    </div>

                </div>
            </section>
            <hr class="m-0" />


        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
